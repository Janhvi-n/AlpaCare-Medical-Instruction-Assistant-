{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpEotmYd8QikyR8Fg6J/v2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Janhvi-n/AlpaCare-Medical-Instruction-Assistant-/blob/main/AlpaCare_Medical_Instruction_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers==4.57.0 datasets accelerate peft -q"
      ],
      "metadata": {
        "id": "pf5kbJBfY_nB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBzwZ3I6ZW7f",
        "outputId": "15775b49-9cd9-4c57-8ecb-d3275fb9612b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.57.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM,\n",
        "                          DataCollatorForSeq2Seq, TrainingArguments, Trainer)\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch"
      ],
      "metadata": {
        "id": "Lu1JNFgQsyGu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_loader.py\n",
        "from datasets import load_dataset\n",
        "\n",
        "def load_and_preprocess():\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(\"lavita/AlpaCare-MedInstruct-52k\")\n",
        "\n",
        "    # Use only a subset (saves Colab time)\n",
        "    dataset = dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "\n",
        "    # Split into train/val/test (90/5/5)\n",
        "    train_val = dataset.train_test_split(test_size=0.10, seed=42)\n",
        "    val_test = train_val[\"test\"].train_test_split(test_size=0.50, seed=42)\n",
        "\n",
        "    train_data = train_val[\"train\"]\n",
        "    val_data = val_test[\"train\"]\n",
        "    test_data = val_test[\"test\"]\n",
        "\n",
        "    return train_data, val_data, test_data\n"
      ],
      "metadata": {
        "id": "GpR61HgMiXQc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, test_data = load_and_preprocess()\n",
        "\n",
        "# Format function\n",
        "def format_data(example):\n",
        "    return {\n",
        "        \"input_text\": example[\"instruction\"],\n",
        "        \"target_text\": example[\"output\"]\n",
        "    }\n",
        "\n",
        "train_data = train_data.map(format_data)\n",
        "val_data = val_data.map(format_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqqM0jADi1hV",
        "outputId": "230a8943-3584-4355-ba83-d80dd84408c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "9ziW2w_vi5Ng"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05\n",
        ")\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "Gjp_r3cCjZtP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 128\n",
        "def tokenize(batch):\n",
        "    # Input\n",
        "    model_inputs = tokenizer(batch[\"input_text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    # Target/labels\n",
        "    labels = tokenizer(batch[\"target_text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    labels[\"input_ids\"] = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]]\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "train_data = train_data.map(tokenize, batched=True)\n",
        "val_data = val_data.map(tokenize, batched=True)\n",
        "\n",
        "train_data.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
        "val_data.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])"
      ],
      "metadata": {
        "id": "Z9ccwnGUjgI0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "2ZEa-wxfQDO2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    eval_strategy=\"steps\",   # old keyword compatible with 4.56.x\n",
        "    learning_rate=2e-4,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=[]  # disables W&B/TensorBoard\n",
        ")\n"
      ],
      "metadata": {
        "id": "TOBz5xnqjkDI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "VeWr21f2J6NS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "t4GWzxcGjpAB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "c155b12e-057b-45f0-9c92-92cb9acf16ed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3820582753.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [225/225 01:39, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=225, training_loss=0.0, metrics={'train_runtime': 101.2111, 'train_samples_per_second': 17.785, 'train_steps_per_second': 2.223, 'total_flos': 309363867648000.0, 'train_loss': 0.0, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"alpacare_lora_adapter\")\n",
        "tokenizer.save_pretrained(\"alpacare_lora_adapter\")\n",
        "print(\"‚úÖ LoRA adapter saved successfully!\")"
      ],
      "metadata": {
        "id": "Q852NiNrjuSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f70e3b-5da9-425b-8e6b-467c58c58a97"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LoRA adapter saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# üîπ Inference Demo\n",
        "# --------------------------\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load base model + LoRA adapter\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model = PeftModel.from_pretrained(base_model, \"alpacare_lora_adapter\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "def ask(prompt, max_new_tokens=150, temperature=0.7, top_p=0.9):\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH\n",
        "    )\n",
        "    # Move tensors to device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate output using sampling\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,      # enable sampling\n",
        "        top_p=top_p,         # nucleus sampling\n",
        "        temperature=temperature,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return text + \"\\n\\n‚ö†Ô∏è This is educational only ‚Äî please consult a qualified clinician.\"\n",
        "\n",
        "# --------------------------\n",
        "# üîπ Test prompts\n",
        "# --------------------------\n",
        "prompts_list = [\n",
        "    \"What are healthy food options for someone with diabetes?\",\n",
        "    \"Explain how to properly use an inhaler.\",\n",
        "    \"Describe safe hand-washing steps to prevent infection.\"\n",
        "]\n",
        "\n",
        "for p in prompts_list:\n",
        "    print(\"Prompt:\", p)\n",
        "    print(ask(p))\n",
        "    print(\"------------------------------------------------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxmFMavpMtTo",
        "outputId": "7c0faf21-fca3-4257-f809-298e449cc10c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What are healthy food options for someone with diabetes?\n",
            "protein and fruit\n",
            "\n",
            "‚ö†Ô∏è This is educational only ‚Äî please consult a qualified clinician.\n",
            "------------------------------------------------------\n",
            "\n",
            "Prompt: Explain how to properly use an inhaler.\n",
            "The inhaler is designed to provide oxygen inhalation. The inhaler can be used to administer inhalations if the inhaler is used in a deep breath. The inhaler is intended to be used when you are not breathing, and is not intended for people with inhalers.\n",
            "\n",
            "‚ö†Ô∏è This is educational only ‚Äî please consult a qualified clinician.\n",
            "------------------------------------------------------\n",
            "\n",
            "Prompt: Describe safe hand-washing steps to prevent infection.\n",
            "Hand-washing is a safe and effective method of cleaning your hands. Use a soap and water solution to wash your hands.\n",
            "\n",
            "‚ö†Ô∏è This is educational only ‚Äî please consult a qualified clinician.\n",
            "------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "path = \"/content/AlpaCare_Medical_Instruction_Assistant.ipynb\"\n",
        "\n",
        "# Load notebook\n",
        "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "# Remove widgets metadata if present\n",
        "if \"widgets\" in nb[\"metadata\"]:\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "\n",
        "# Save cleaned notebook\n",
        "cleaned_path = \"/content/AlpaCare_Cleaned.ipynb\"\n",
        "with open(cleaned_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "print(\"‚úÖ Notebook cleaned successfully! Ready to upload to GitHub.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFeIXgHyhaW1",
        "outputId": "e1820bdd-274d-4653-b535-fa40c754e434"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Notebook cleaned successfully! Ready to upload to GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IUsS1CY5iN1D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}